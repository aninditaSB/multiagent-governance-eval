{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53b1f51c",
   "metadata": {},
   "source": [
    "\n",
    "## Multi-Agent System on SQuAD with Centralized, Distributed, and Hybrid Governance\n",
    "\n",
    "This notebook contains experiments for a multi-agent system built on the SQuAD dataset. It explores how different governance strategies—**centralized**, **distributed**, and **hybrid**—affect the performance and safety of large language model (LLM)-based agents.\n",
    "\n",
    "### About the Dataset: Why SQuAD?\n",
    "\n",
    "We chose the **Stanford Question Answering Dataset (SQuAD)** because it’s a trusted benchmark for evaluating question-answering systems. It contains over 100,000 questions paired with Wikipedia passages and gold-standard answers.\n",
    "\n",
    "SQuAD is ideal for testing multi-agent systems because:\n",
    "\n",
    "* It covers a wide variety of factual, text-based questions.\n",
    "* Answers are context-grounded and span-based, making evaluation easier and more accurate.\n",
    "* It reflects tasks like retrieval, reasoning, and summarization—core skills for agentic systems.\n",
    "\n",
    "### System Design: The Three Core Agents\n",
    "\n",
    "Our system breaks down the QA task into three modular agents:\n",
    "\n",
    "* **ClassifierAgent** – Detects the question type (e.g., who, what, when) to guide how the question is handled.\n",
    "* **RAGAgent** – Uses retrieval-augmented generation to find relevant information and generate an answer.\n",
    "* **SummarizerAgent** – (Optional) Summarizes longer answers for clarity and readability.\n",
    "\n",
    "This design reflects how people solve complex problems—by breaking them into focused steps—and allows us to analyze each part individually.\n",
    "\n",
    "### Governance Architectures: How Control is Applied\n",
    "\n",
    "We implement and compare three governance models for managing agent behavior:\n",
    "\n",
    "#### 1. Centralized Governance\n",
    "\n",
    "* **How it works**: The system completes all agent steps, then runs quality checks only at the end.\n",
    "* **Key feature**: A single controller reviews the final answer, like a teacher grading a test.\n",
    "* **Purpose**: Acts as a baseline to measure the effects of having full control at one point.\n",
    "\n",
    "#### 2. Distributed Governance\n",
    "\n",
    "* **How it works**: Each agent checks its own output before passing it on. If the output fails, it retries or stops the process.\n",
    "* **Key feature**: There’s no central controller—each agent is responsible for its own quality.\n",
    "* **Purpose**: Models decentralized, peer-like agent systems where each module is self-accountable.\n",
    "\n",
    "#### 3. Hybrid Governance\n",
    "\n",
    "* **How it works**: A central controller decides when and where to apply checks based on confidence scores.\n",
    "\n",
    "  * If the system is confident, it runs fewer checks.\n",
    "  * If uncertain, it adds extra evaluations (e.g., transparency or summarization).\n",
    "* **Key feature**: A balance between the other two models—adaptive, not fixed.\n",
    "* **Purpose**: Reflects real-world systems that optimize resources by checking more only when needed.\n",
    "\n",
    "### Evaluation and Guardrails\n",
    "\n",
    "We used **DeepEval** to implement guardrails—automated quality checks—for each governance mode. These guardrails evaluated:\n",
    "\n",
    "* Answer relevance\n",
    "* Task completion\n",
    "* Transparency\n",
    "* Helpfulness\n",
    "\n",
    "In **distributed** and **hybrid** modes, these metrics were applied at the agent level, allowing dynamic responses like retrying or escalating low-confidence answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "debe7f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/legal_multi_agents/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import nest_asyncio; nest_asyncio.apply()  # Needed for notebook async loop fixes\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.tools import QueryEngineTool, FunctionTool\n",
    "from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e45c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GOOGLE_API_KEY\")\n",
    "assert GOOGLE_API_KEY, \"Set your Google API key!\"\n",
    "\n",
    "# LLM and Embedding Model\n",
    "llm = GoogleGenAI(model=\"models/gemini-2.5-pro\", api_key=GOOGLE_API_KEY)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c72d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DeepEval Metrics ---\n",
    "from deepeval.models import GeminiModel\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.models import GeminiModel\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    GEval,\n",
    "    TaskCompletionMetric\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e882deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepEval model/metrics\n",
    "eval_model = GeminiModel(model_name=\"gemini-2.5-pro\", api_key=GOOGLE_API_KEY)\n",
    "transparency_metric = GEval(\n",
    "    model=eval_model,\n",
    "    name=\"Transparency\",\n",
    "    criteria=\"Does the answer clearly explain its reasoning steps and tool usage? Rate 0.0–1.0.\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT],\n",
    "    threshold=0.35,\n",
    ")\n",
    "helpfulness_metric = GEval(\n",
    "    model=eval_model,\n",
    "    name=\"Helpfulness\",\n",
    "    criteria=\"How helpful is this answer to the user's question? Rate 0.0-1.0.\",\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.35,\n",
    ")\n",
    "task_metric = TaskCompletionMetric(model=eval_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81cfbc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_answer_relevancy = AnswerRelevancyMetric(model=eval_model, threshold=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "951adc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG corpus size: 10000 | Eval set: 2000 Qs (all context in index)\n"
     ]
    }
   ],
   "source": [
    "# 1. Load and shuffle SQuAD train split\n",
    "#full_train = load_dataset(\"squad\", split=\"train[:10000]\").shuffle(seed=42)\n",
    "\n",
    "#eval_size = 1000  # 10% for eval\n",
    "\n",
    "full_train = load_dataset(\"squad\", split=\"train[:12000]\").shuffle(seed=42)\n",
    "eval_size = 2000  # Now using 2,000 for evaluation\n",
    "\n",
    "\n",
    "index_size = len(full_train) - eval_size\n",
    "\n",
    "# 2. Build RAG corpus from first 9000 (use select)\n",
    "rag_docs = [Document(text=d[\"context\"]) for d in full_train.select(range(index_size))]\n",
    "\n",
    "# 3. Build eval set from held-out 1000\n",
    "eval_examples = full_train.select(range(index_size, len(full_train)))\n",
    "\n",
    "questions = [d[\"question\"] for d in eval_examples]\n",
    "golds = [d[\"answers\"][\"text\"][0] if d[\"answers\"][\"text\"] else \"\" for d in eval_examples]\n",
    "eval_contexts = [d[\"context\"] for d in eval_examples]  # For debugging\n",
    "\n",
    "print(f\"RAG corpus size: {len(rag_docs)} | Eval set: {len(questions)} Qs (all context in index)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f2b0686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RAG index and retriever ---\n",
    "docs_index = VectorStoreIndex.from_documents(rag_docs, embed_model=embed_model)\n",
    "query_engine = docs_index.as_query_engine(llm=llm)\n",
    "rag_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine, name=\"RAGAgent\", description=\"Retrieval-augmented generation for SQuAD.\"\n",
    ")\n",
    "retriever = docs_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e20dca",
   "metadata": {},
   "source": [
    "# DISTRIBUTED - independent agents with guardrails at all agent levels - no central controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dda5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nest_asyncio; nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2177a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Build RAG index and query engine ---\n",
    "#docs_index = VectorStoreIndex.from_documents(rag_docs, embed_model=embed_model)\n",
    "retriever = docs_index.as_retriever(similarity_top_k=10)  # Increased top_k for more/better context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25956681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_prompt(question, context_snippets):\n",
    "    context = \"\\n\".join([f\"[Source {i+1}]: {snippet}\" for i, snippet in enumerate(context_snippets)])\n",
    "    prompt = f'''\n",
    "You are a question answering system. Use the information in the SOURCES below to answer the question as completely and accurately as possible.\n",
    "- Cite supporting facts with [Source #] after each claim.\n",
    "- If the answer cannot be found in the sources, reply: \"Not found in context.\"\n",
    "- Keep your answer concise but complete; do not copy sources verbatim unless necessary.\n",
    "\n",
    "SOURCES:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER (with citations):\n",
    "'''\n",
    "    return prompt\n",
    "\n",
    "def verify_citations(answer, context_snippets):\n",
    "    import re\n",
    "    for match in re.finditer(r\"\\[Source (\\d+)\\]\", answer):\n",
    "        idx = int(match.group(1)) - 1\n",
    "        if idx < 0 or idx >= len(context_snippets):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def deepeval_guardrail(answer: str, question: str, retrieval_context: list = None):\n",
    "    test_case = LLMTestCase(\n",
    "        input=question,\n",
    "        actual_output=answer,\n",
    "        retrieval_context=retrieval_context or []\n",
    "    )\n",
    "    helpfulness = float(helpfulness_metric.measure(test_case))\n",
    "    answer_relevancy = float(metric_answer_relevancy.measure(test_case))\n",
    "    citations_ok = verify_citations(answer, retrieval_context or [])\n",
    "    if helpfulness < 0.35:\n",
    "        return f\"FAIL: Helpfulness below threshold ({helpfulness:.2f})\"\n",
    "    if answer_relevancy < 0.8:\n",
    "        return f\"FAIL: AnswerRelevancy below threshold ({answer_relevancy:.2f})\"\n",
    "    if not citations_ok:\n",
    "        return \"FAIL: Citation(s) do not match provided sources\"\n",
    "    return \"PASS\"\n",
    "\n",
    "def rag_generate(question, retrieved_nodes):\n",
    "    context_snippets = [str(n) for n in retrieved_nodes]\n",
    "    prompt = rag_prompt(question, context_snippets)\n",
    "    response = llm.complete(prompt)\n",
    "    if hasattr(response, 'text'):\n",
    "        return response.text\n",
    "    return str(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1c12308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Q1 added. Gold answer is present in context.\n",
      "[OK] Q2 added. Gold answer is present in context.\n",
      "[SKIP] Q3: Gold NOT found in context.\n",
      "Gold: Portuguese Football Federation (FPF)  – Federação Portuguesa de Futebol\n",
      "\n",
      "[OK] Q4 added. Gold answer is present in context.\n",
      "[SKIP] Q5: Gold NOT found in context.\n",
      "Gold: nearly 1,500\n",
      "\n",
      "[OK] Q6 added. Gold answer is present in context.\n",
      "[OK] Q7 added. Gold answer is present in context.\n",
      "[OK] Q8 added. Gold answer is present in context.\n",
      "[SKIP] Q9: Gold NOT found in context.\n",
      "Gold: Island Def Jam\n",
      "\n",
      "[OK] Q10 added. Gold answer is present in context.\n",
      "[OK] Q11 added. Gold answer is present in context.\n",
      "[SKIP] Q12: Gold NOT found in context.\n",
      "Gold: John Logan\n",
      "\n",
      "[OK] Q13 added. Gold answer is present in context.\n",
      "[OK] Q14 added. Gold answer is present in context.\n",
      "[OK] Q15 added. Gold answer is present in context.\n",
      "[OK] Q16 added. Gold answer is present in context.\n",
      "[SKIP] Q17: Gold NOT found in context.\n",
      "Gold: Eastern\n",
      "\n",
      "[OK] Q18 added. Gold answer is present in context.\n",
      "[OK] Q19 added. Gold answer is present in context.\n",
      "[OK] Q20 added. Gold answer is present in context.\n",
      "[OK] Q21 added. Gold answer is present in context.\n",
      "[SKIP] Q22: Gold NOT found in context.\n",
      "Gold: 45 BC until 298 AD\n",
      "\n",
      "[OK] Q23 added. Gold answer is present in context.\n",
      "[OK] Q24 added. Gold answer is present in context.\n",
      "[SKIP] Q25: Gold NOT found in context.\n",
      "Gold: District of Colorado\n",
      "\n",
      "[SKIP] Q26: Gold NOT found in context.\n",
      "Gold: \"quality\", \"matter\", and \"form\"\n",
      "\n",
      "[SKIP] Q27: Gold NOT found in context.\n",
      "Gold: Nuclear\n",
      "\n",
      "[OK] Q28 added. Gold answer is present in context.\n",
      "[OK] Q29 added. Gold answer is present in context.\n",
      "[OK] Q30 added. Gold answer is present in context.\n",
      "[OK] Q31 added. Gold answer is present in context.\n",
      "[SKIP] Q32: Gold NOT found in context.\n",
      "Gold: 20\n",
      "\n",
      "[OK] Q33 added. Gold answer is present in context.\n",
      "[OK] Q34 added. Gold answer is present in context.\n",
      "[OK] Q35 added. Gold answer is present in context.\n",
      "[SKIP] Q36: Gold NOT found in context.\n",
      "Gold: 15\n",
      "\n",
      "[OK] Q37 added. Gold answer is present in context.\n",
      "[SKIP] Q38: Gold NOT found in context.\n",
      "Gold: 1847\n",
      "\n",
      "[SKIP] Q39: Gold NOT found in context.\n",
      "Gold: 250\n",
      "\n",
      "[OK] Q40 added. Gold answer is present in context.\n",
      "[OK] Q41 added. Gold answer is present in context.\n",
      "[SKIP] Q42: Gold NOT found in context.\n",
      "Gold: Jan Matuszyński and Julian Fontana\n",
      "\n",
      "[SKIP] Q43: Gold NOT found in context.\n",
      "Gold: Jin Jing\n",
      "\n",
      "[OK] Q44 added. Gold answer is present in context.\n",
      "[SKIP] Q45: Gold NOT found in context.\n",
      "Gold: three\n",
      "\n",
      "[OK] Q46 added. Gold answer is present in context.\n",
      "[OK] Q47 added. Gold answer is present in context.\n",
      "[SKIP] Q48: Gold NOT found in context.\n",
      "Gold: Ponte Sisto bridge and the Roman Forum\n",
      "\n",
      "[SKIP] Q49: Gold NOT found in context.\n",
      "Gold: Congress\n",
      "\n",
      "[SKIP] Q50: Gold NOT found in context.\n",
      "Gold: people's motives and behavior\n",
      "\n",
      "[OK] Q51 added. Gold answer is present in context.\n",
      "[OK] Q52 added. Gold answer is present in context.\n",
      "[SKIP] Q53: Gold NOT found in context.\n",
      "Gold: Temple's Gifford Lectures of 1932-1934\n",
      "\n",
      "[OK] Q54 added. Gold answer is present in context.\n",
      "[OK] Q55 added. Gold answer is present in context.\n",
      "[SKIP] Q56: Gold NOT found in context.\n",
      "Gold: Tenure of Office Act\n",
      "\n",
      "[SKIP] Q57: Gold NOT found in context.\n",
      "Gold: De architectura\n",
      "\n",
      "[SKIP] Q58: Gold NOT found in context.\n",
      "Gold: Good Water Store and Café\n",
      "\n",
      "[OK] Q59 added. Gold answer is present in context.\n",
      "[OK] Q60 added. Gold answer is present in context.\n",
      "[OK] Q61 added. Gold answer is present in context.\n",
      "[OK] Q62 added. Gold answer is present in context.\n",
      "[OK] Q63 added. Gold answer is present in context.\n",
      "[OK] Q64 added. Gold answer is present in context.\n",
      "[OK] Q65 added. Gold answer is present in context.\n",
      "[OK] Q66 added. Gold answer is present in context.\n",
      "[OK] Q67 added. Gold answer is present in context.\n",
      "[SKIP] Q68: Gold NOT found in context.\n",
      "Gold: Aníbal Cavaco Silva\n",
      "\n",
      "[OK] Q69 added. Gold answer is present in context.\n",
      "[OK] Q70 added. Gold answer is present in context.\n",
      "[OK] Q71 added. Gold answer is present in context.\n",
      "[OK] Q72 added. Gold answer is present in context.\n",
      "[SKIP] Q73: Gold NOT found in context.\n",
      "Gold: on the ground\n",
      "\n",
      "[OK] Q74 added. Gold answer is present in context.\n",
      "[OK] Q75 added. Gold answer is present in context.\n",
      "\n",
      "Selected 50 eval Qs out of 2000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "# 1. Normalize text and robustly check for answer in context\n",
    "def normalize_text(text):\n",
    "    \"\"\"Lowercase, remove punctuation, extra whitespace.\"\"\"\n",
    "    return re.sub(r'[\\W_]+', ' ', text.lower()).strip()\n",
    "\n",
    "def answer_in_context(gold, rag_context, min_frac=0.7):\n",
    "    \"\"\"\n",
    "    Returns True if gold answer is present (even partially) in any context snippet.\n",
    "    min_frac: minimum fraction of gold words found in context for robust match.\n",
    "    \"\"\"\n",
    "    gold_norm = normalize_text(gold)\n",
    "    gold_words = set(gold_norm.split())\n",
    "    if not gold_words:\n",
    "        return False\n",
    "    for snippet in rag_context:\n",
    "        snippet_norm = normalize_text(snippet)\n",
    "        # 1. Exact match (normalized)\n",
    "        if gold_norm in snippet_norm:\n",
    "            return True\n",
    "        # 2. Loose/partial match: most gold words are in context\n",
    "        snippet_words = set(snippet_norm.split())\n",
    "        common = gold_words & snippet_words\n",
    "        if len(common) / len(gold_words) >= min_frac:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 2. Filter eval set for only those Qs with gold answer in retrieved context\n",
    "selected_questions, selected_golds = [], []\n",
    "for i, (q, g) in enumerate(zip(questions, golds)):\n",
    "    nodes = retriever.retrieve(q)\n",
    "    rag_context = [str(n) for n in nodes]\n",
    "    if answer_in_context(g, rag_context, min_frac=0.7):\n",
    "        selected_questions.append(q)\n",
    "        selected_golds.append(g)\n",
    "        print(f\"[OK] Q{i+1} added. Gold answer is present in context.\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Q{i+1}: Gold NOT found in context.\\nGold: {g}\\n\")\n",
    "    #if len(selected_questions) >= 20:   # <-- How many eval Qs you want\n",
    "    #    break\n",
    "    if len(selected_questions) >= 50:   # <-- How many eval Qs you want\n",
    "        break\n",
    "\n",
    "print(f\"\\nSelected {len(selected_questions)} eval Qs out of {len(questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bec25f",
   "metadata": {},
   "source": [
    "#run for 50 selected questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "387f0cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Q1: Who was first to sequence a DNA-based genome?\n",
      "========================================\n",
      "  [Retry 0] RAG context retrieved, confidence: 0.64\n",
      "    [Context snippets]:\n",
      "      [1] Node ID: 3d7dd8f4-8a9b-4ec5-a29c-58f6cb326d88 Text: In 1976, Walter Fiers at the University of Ghent (Belgium) was the first to establish the complete nucleotide sequence of a vira...\n",
      "      [2] Node ID: 1b3b67ed-d61d-4cd0-b2ce-ffaeccd8b821 Text: New sequencing technologies, such as massive parallel sequencing have also opened up the prospect of personal genome sequencing ...\n",
      "      [3] Node ID: 73461e84-73fb-4900-bd9d-fc8ab65b0c3b Text: The development of new technologies has made it dramatically easier and cheaper to do sequencing, and the number of complete gen...\n",
      "    [RAG Prompt to LLM]:\n",
      "\n",
      "You are a question answering system. Use the information in the SOURCES below to answer the question as completely and accurately as possible.\n",
      "- Cite supporting facts with [Source #] after each claim.\n",
      "- If the answer cannot be found in the sources, reply: \"Not found in context.\"\n",
      "- Keep your answer concise but complete; do not copy sources verbatim unless necessary.\n",
      "\n",
      "SOURCES:\n",
      "[Source 1]: Node ID: 3d7dd8f4-8a9b-4ec5-a29c-58f6cb326d88\n",
      "Text: In 1976, Walter Fiers at the University of Ghent (Belgium) was\n",
      "the first to establish the complete nucleotide sequence of a viral\n",
      "RNA-genome (Bacteriophage M...\n",
      "--- END PROMPT ---\n",
      "    [RAG LLM] Agent answer generated in 9.15s\n",
      "    [LLM Answer]:\n",
      "Fred Sanger completed the first DNA-genome sequence in 1977 [Source 1]. The sequence was of the Phage Φ-X174 genome, which has 5,386 base pairs [Source 1]. This achievement followed the first sequencing of an RNA-genome by Walter Fiers in 1976 [Source 1]....\n",
      "--- END ANSWER ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [Guardrail] Result: FAIL: AnswerRelevancy below threshold (0.75) (checked in 34.58s)\n",
      "    [Retry] Guardrail failed or RAG conf too low; retrying...\n",
      "  [Retry 1] Step runtime: 44.15s\n",
      "  [Retry 1] RAG context retrieved, confidence: 0.64\n",
      "    [Context snippets]:\n",
      "      [1] Node ID: 3d7dd8f4-8a9b-4ec5-a29c-58f6cb326d88 Text: In 1976, Walter Fiers at the University of Ghent (Belgium) was the first to establish the complete nucleotide sequence of a vira...\n",
      "      [2] Node ID: 1b3b67ed-d61d-4cd0-b2ce-ffaeccd8b821 Text: New sequencing technologies, such as massive parallel sequencing have also opened up the prospect of personal genome sequencing ...\n",
      "      [3] Node ID: 73461e84-73fb-4900-bd9d-fc8ab65b0c3b Text: The development of new technologies has made it dramatically easier and cheaper to do sequencing, and the number of complete gen...\n",
      "    [RAG Prompt to LLM]:\n",
      "\n",
      "You are a question answering system. Use the information in the SOURCES below to answer the question as completely and accurately as possible.\n",
      "- Cite supporting facts with [Source #] after each claim.\n",
      "- If the answer cannot be found in the sources, reply: \"Not found in context.\"\n",
      "- Keep your answer concise but complete; do not copy sources verbatim unless necessary.\n",
      "\n",
      "SOURCES:\n",
      "[Source 1]: Node ID: 3d7dd8f4-8a9b-4ec5-a29c-58f6cb326d88\n",
      "Text: In 1976, Walter Fiers at the University of Ghent (Belgium) was\n",
      "the first to establish the complete nucleotide sequence of a viral\n",
      "RNA-genome (Bacteriophage M...\n",
      "--- END PROMPT ---\n",
      "    [RAG LLM] Agent answer generated in 9.79s\n",
      "    [LLM Answer]:\n",
      "Fred Sanger was the first to sequence a DNA-based genome [Source 1]. In 1977, he completed the sequence for Phage Φ-X174, which consists of 5,386 base pairs [Source 1]. This achievement followed Walter Fiers's 1976 sequencing of the first viral RNA-genome [Source 1]....\n",
      "--- END ANSWER ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [Guardrail] Result: FAIL: AnswerRelevancy below threshold (0.67) (checked in 42.76s)\n",
      "    [Retry] Guardrail failed or RAG conf too low; retrying...\n",
      "  [Retry 2] Step runtime: 52.69s\n",
      "    [Gold Answer]: Fred Sanger\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [Metrics] helpfulness: 1.00, transparency: 0.20, task_completion: 1.00, answer_relevancy: 0.75\n",
      "Q1 done. RAG Conf: 0.64 (total 96.84s)\n",
      "\n",
      "=== Q2: When was a resolution agreed to about Chinese human rights issues in San Francisco?\n",
      "========================================\n",
      "  [Retry 0] RAG context retrieved, confidence: 0.63\n",
      "    [Context snippets]:\n",
      "      [1] Node ID: 6101ba51-4049-4520-9fe9-04c86270e8fd Text: On April 1, 2008, the San Francisco Board of Supervisors approved a resolution addressing human rights concerns when the Beijing...\n",
      "      [2] Node ID: 072f98f4-bea0-43d0-8265-8e5052a8b356 Text: Some advocates for Tibet, Darfur, and the spiritual practice Falun Gong, planned to protest the April 9 arrival of the torch in ...\n",
      "    [RAG Prompt to LLM]:\n",
      "\n",
      "You are a question answering system. Use the information in the SOURCES below to answer the question as completely and accurately as possible.\n",
      "- Cite supporting facts with [Source #] after each claim.\n",
      "- If the answer cannot be found in the sources, reply: \"Not found in context.\"\n",
      "- Keep your answer concise but complete; do not copy sources verbatim unless necessary.\n",
      "\n",
      "SOURCES:\n",
      "[Source 1]: Node ID: 6101ba51-4049-4520-9fe9-04c86270e8fd\n",
      "Text: On April 1, 2008, the San Francisco Board of Supervisors\n",
      "approved a resolution addressing human rights concerns when the\n",
      "Beijing Olympic torch arrives in San...\n",
      "--- END PROMPT ---\n",
      "    [RAG LLM] Agent answer generated in 9.02s\n",
      "    [LLM Answer]:\n",
      "On April 1, 2008, the San Francisco Board of Supervisors approved a resolution addressing human rights concerns related to China [Source 1]. The resolution was passed in anticipation of the Beijing Olympic torch's arrival in the city on April 9 [Source 1, Source 2]....\n",
      "--- END ANSWER ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [Guardrail] Result: PASS (checked in 42.01s)\n",
      "  [Retry 0] Step runtime: 51.22s\n",
      "    [Gold Answer]: April 1, 2008\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [Metrics] helpfulness: 1.00, transparency: 0.00, task_completion: 1.00, answer_relevancy: 1.00\n",
      "Q2 done. RAG Conf: 0.63 (total 51.22s)\n",
      "\n",
      "Table row: | Distributed | helpfulness: 1.00 | transparency: 0.10 | task_completion: 1.00 | answer_relevancy: 0.88 | RAG Conf.: 0.64 | Runtime: 74.03 |\n",
      "\n",
      "Saved distributed_deepeval_guardrail_results_ICAIR.json\n"
     ]
    }
   ],
   "source": [
    "# 3. Run distributed governance on the filtered eval set\n",
    "\n",
    "async def run_distributed_squad(n=None, max_retries=1, rag_conf_threshold=0.35):\n",
    "    results = []\n",
    "    initial_top_k = retriever.similarity_top_k  # Save at start\n",
    "    use_n = n if n else len(selected_questions)\n",
    "\n",
    "    # Define all metrics (including new ones)\n",
    "    metrics = {\n",
    "        \"helpfulness\": helpfulness_metric,\n",
    "        \"transparency\": transparency_metric,\n",
    "        \"task_completion\": task_metric,\n",
    "        \"answer_relevancy\": metric_answer_relevancy,\n",
    "        \n",
    "    }\n",
    "\n",
    "    for i, (q, g) in enumerate(zip(selected_questions, selected_golds)):\n",
    "        if i >= use_n:\n",
    "            break\n",
    "        print(f\"\\n=== Q{i+1}: {q}\\n{'='*40}\")\n",
    "        t0 = time.time()\n",
    "        retries = 0\n",
    "        agent_answer = None\n",
    "        rag_context = []\n",
    "        rag_conf = 0.0\n",
    "        decision = \"retry\"\n",
    "        last_top_k = retriever.similarity_top_k\n",
    "\n",
    "        while decision != \"accept\" and retries <= max_retries:\n",
    "            step_start = time.time()\n",
    "            # ---- Retrieval Step ----\n",
    "            nodes = retriever.retrieve(q)\n",
    "            rag_context = [str(n) for n in nodes]\n",
    "            rag_conf = max(getattr(n, \"score\", 0.0) for n in nodes) if nodes else 0.0\n",
    "            print(f\"  [Retry {retries}] RAG context retrieved, confidence: {rag_conf:.2f}\")\n",
    "\n",
    "            print(\"    [Context snippets]:\")\n",
    "            for idx, snippet in enumerate(rag_context):\n",
    "                snippet_preview = snippet[:180].replace('\\n', ' ')\n",
    "                print(f\"      [{idx+1}] {snippet_preview}...\")\n",
    "\n",
    "            # Dynamically increase top_k, but reset per question\n",
    "            if rag_conf < rag_conf_threshold and retries < max_retries:\n",
    "                retriever.similarity_top_k += 2\n",
    "                if retriever.similarity_top_k != last_top_k:\n",
    "                    print(f\"    [Info] Increased retriever.similarity_top_k to {retriever.similarity_top_k}\")\n",
    "                    last_top_k = retriever.similarity_top_k\n",
    "\n",
    "            # ---- RAG Agent Step ----\n",
    "            agent_start = time.time()\n",
    "            rag_prompt_str = rag_prompt(q, rag_context)\n",
    "            print(f\"    [RAG Prompt to LLM]:\\n{rag_prompt_str[:600]}...\\n--- END PROMPT ---\")\n",
    "            agent_answer = rag_generate(q, nodes)\n",
    "            agent_end = time.time()\n",
    "            print(f\"    [RAG LLM] Agent answer generated in {agent_end-agent_start:.2f}s\")\n",
    "            print(f\"    [LLM Answer]:\\n{str(agent_answer)[:400]}...\\n--- END ANSWER ---\")\n",
    "            \n",
    "            # ---- Guardrail Step ----\n",
    "            guardrail_start = time.time()\n",
    "            guardrail_result = deepeval_guardrail(agent_answer, q, rag_context)\n",
    "            guardrail_end = time.time()\n",
    "            print(f\"    [Guardrail] Result: {guardrail_result} (checked in {guardrail_end-guardrail_start:.2f}s)\")\n",
    "            if guardrail_result == \"PASS\" and rag_conf >= rag_conf_threshold:\n",
    "                decision = \"accept\"\n",
    "            else:\n",
    "                print(f\"    [Retry] Guardrail failed or RAG conf too low; retrying...\")\n",
    "                retries += 1\n",
    "            step_end = time.time()\n",
    "            print(f\"  [Retry {retries}] Step runtime: {step_end-step_start:.2f}s\")\n",
    "\n",
    "        t1 = time.time()\n",
    "        retriever.similarity_top_k = initial_top_k  # Reset for next question\n",
    "        print(f\"    [Gold Answer]: {g}\\n\")\n",
    "\n",
    "        # ---- Metrics Calculation ----\n",
    "        test_case = LLMTestCase(\n",
    "            input=q,\n",
    "            actual_output=agent_answer,\n",
    "            expected_output=g,\n",
    "            retrieval_context=rag_context,\n",
    "            tools_called=[], expected_tools=[]\n",
    "        )\n",
    "        metric_scores = {}\n",
    "        for name, metric in metrics.items():\n",
    "            score = float(metric.measure(test_case))\n",
    "            metric_scores[name] = score\n",
    "        metric_str = \", \".join(f\"{k}: {v:.2f}\" for k, v in metric_scores.items())\n",
    "        print(f\"    [Metrics] {metric_str}\")\n",
    "\n",
    "        results.append({\n",
    "            \"index\": i+1,\n",
    "            \"query\": q,\n",
    "            \"gold_answer\": g,\n",
    "            \"agent_answer\": agent_answer,\n",
    "            \"retrieved_context\": rag_context,\n",
    "            \"rag_confidence\": rag_conf,\n",
    "            \"decision\": decision,\n",
    "            \"runtime\": t1 - t0,\n",
    "            **metric_scores  # Inject all metric values directly into the result dict\n",
    "        })\n",
    "        print(f\"Q{i+1} done. RAG Conf: {rag_conf:.2f} (total {t1-t0:.2f}s)\")\n",
    "\n",
    "    # ---- Table Stats (Dynamic Metrics) ----\n",
    "    avg_metrics = {\n",
    "        name: sum(d[name] for d in results) / len(results)\n",
    "        for name in metrics\n",
    "    }\n",
    "    avg_ragconf = sum(d['rag_confidence'] for d in results) / len(results)\n",
    "    avg_rt = sum(d['runtime'] for d in results) / len(results)\n",
    "\n",
    "    metric_values_str = \" | \".join(f\"{m}: {avg_metrics[m]:.2f}\" for m in metrics)\n",
    "    print(f\"\\nTable row: | Distributed | {metric_values_str} | RAG Conf.: {avg_ragconf:.2f} | Runtime: {avg_rt:.2f} |\")\n",
    "\n",
    "    # ---- Save Results ----\n",
    "    with open(\"distributed_deepeval_guardrail_results_ICAIR.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    print(\"\\nSaved distributed_deepeval_guardrail_results_ICAIR.json\")\n",
    "\n",
    "# 4. RUN!\n",
    "await run_distributed_squad(max_retries=1, rag_conf_threshold=0.35) #FINAL\n",
    "#await run_distributed_squad(n=2, max_retries=1, rag_conf_threshold=0.35) # temporary for testing- to remove 2 aug\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a84a6ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Final Results (Averages over 2 samples) ====\n",
      "Helpfulness: 1.00\n",
      "Transparency: 0.10\n",
      "Task Completion: 1.00\n",
      "Answer Relevancy: 0.88\n",
      "Avg. RAG Conf.: 0.64\n",
      "Avg. Runtime (s): 74.03\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load  results JSON\n",
    "with open(\"distributed_deepeval_guardrail_results_ICAIR.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# The metrics to average\n",
    "metrics = [\n",
    "    \"helpfulness\",\n",
    "    \"transparency\",\n",
    "    \"task_completion\",\n",
    "    \"answer_relevancy\",\n",
    "    \"rag_confidence\",\n",
    "    \"runtime\"\n",
    "]\n",
    "\n",
    "# Compute averages (protect against missing keys)\n",
    "avg = {}\n",
    "N = len(results)\n",
    "for m in metrics:\n",
    "    vals = [r.get(m, 0.0) for r in results]\n",
    "    avg[m] = sum(vals) / N if N else 0.0\n",
    "\n",
    "# Print in table format\n",
    "print(\"==== Final Results (Averages over {} samples) ====\".format(N))\n",
    "print(\"Helpfulness: {:.2f}\".format(avg[\"helpfulness\"]))\n",
    "print(\"Transparency: {:.2f}\".format(avg[\"transparency\"]))\n",
    "print(\"Task Completion: {:.2f}\".format(avg[\"task_completion\"]))\n",
    "print(\"Answer Relevancy: {:.2f}\".format(avg[\"answer_relevancy\"]))\n",
    "print(\"Avg. RAG Conf.: {:.2f}\".format(avg[\"rag_confidence\"]))\n",
    "print(\"Avg. Runtime (s): {:.2f}\".format(avg[\"runtime\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c261f",
   "metadata": {},
   "source": [
    "\n",
    "# CENTRALIZED GOVERANCE\n",
    "- Centralised governance = run the agentic flow end-to-end, get the final eval and decide what to do.\n",
    "- centralized is like a teacher evaluting our final output of all agents and the way they solved the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5a6f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer_prompt(answer_with_sources, question):\n",
    "    return (\n",
    "        \"Summarize the answer below, preserving all source citations.\\n\\n\"\n",
    "        f\"Answer: {answer_with_sources}\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        \"Summary:\"\n",
    "    )\n",
    "\n",
    "def summarize_answer(answer_with_sources, question):\n",
    "    \"\"\"\n",
    "    Summarizes the RAG answer, preserving citations and using the original question for clarity.\n",
    "    \"\"\"\n",
    "    prompt = summarizer_prompt(answer_with_sources, question)\n",
    "    response = llm.complete(prompt)\n",
    "    if hasattr(response, 'text'):\n",
    "        return response.text\n",
    "    return str(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4432742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "# --- Centralized governance (run full pipeline then evaluate once) ---\n",
    "def centralized_governance(question: str, gold: str) -> dict:\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # 1. RAG Retrieval\n",
    "    nodes = retriever.retrieve(question)\n",
    "    rag_context = [str(n) for n in nodes]\n",
    "    rag_conf = max(getattr(n, \"score\", 0.0) for n in nodes) if nodes else 0.0\n",
    "\n",
    "    # 2. RAG Generation\n",
    "    rag_answer = rag_generate(question, nodes)\n",
    "\n",
    "    # 3. Summarization (if you want to summarize before evaluation)\n",
    "    try:\n",
    "        summary = summarize_answer(rag_answer, question)\n",
    "    except Exception:\n",
    "        summary = rag_answer   # fallback\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    # 4. Evaluation (centralized, on summary)\n",
    "    test_case = LLMTestCase(\n",
    "        input=question,\n",
    "        actual_output=summary,\n",
    "        expected_output=gold,\n",
    "        retrieval_context=rag_context,\n",
    "        tools_called=[], expected_tools=[]\n",
    "    )\n",
    "    output = {\n",
    "        \"query\": question,\n",
    "        \"gold_answer\": gold,\n",
    "        \"response\": summary,\n",
    "        \"retrieved_context\": rag_context,\n",
    "        \"rag_confidence\": rag_conf,\n",
    "        \"runtime\": t1-t0,\n",
    "        \"helpfulness\": float(helpfulness_metric.measure(test_case)),\n",
    "        \"transparency\": float(transparency_metric.measure(test_case)),\n",
    "        \"task_completion\": float(task_metric.measure(test_case)),\n",
    "        \"answer_relevancy\": float(metric_answer_relevancy.measure(test_case)),\n",
    "    }\n",
    "    \n",
    "    # All-or-nothing guardrail (centralized)\n",
    "    passed = (\n",
    "        output[\"helpfulness\"] >= helpfulness_metric.threshold and\n",
    "        output[\"transparency\"] >= transparency_metric.threshold and\n",
    "        output[\"task_completion\"] >= task_metric.threshold\n",
    "    )\n",
    "    output[\"decision\"] = \"accept\" if passed else \"retry\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "# ---- Batch runner ----\n",
    "results = []\n",
    "for i, (q, g) in enumerate(zip(selected_questions, selected_golds)):\n",
    "    print(f\"\\n=== Q{i+1}: {q}\\n{'='*40}\")\n",
    "    out = centralized_governance(q, g)\n",
    "    print(f\"  [Gold]: {g}\")\n",
    "    print(f\"  [Pred]: {out['response']}\")\n",
    "    print(\n",
    "        f\"  [Help]: {out['helpfulness']:.2f} | [Transp]: {out['transparency']:.2f} | \"\n",
    "        f\"[Task]: {out['task_completion']:.2f} | [Rel]: {out['answer_relevancy']:.2f} | \"\n",
    "        f\"[RAG Conf]: {out['rag_confidence']:.2f} | [Decision]: {out['decision']}\"\n",
    "    )\n",
    "    results.append(out)\n",
    "\n",
    "# --- Print summary table ---\n",
    "n = len(results)\n",
    "avg_help = sum(r['helpfulness'] for r in results) / n\n",
    "avg_transp = sum(r['transparency'] for r in results) / n\n",
    "avg_task = sum(r['task_completion'] for r in results) / n\n",
    "avg_relevancy = sum(r['answer_relevancy'] for r in results) / n\n",
    "avg_ragconf = sum(r['rag_confidence'] for r in results) / n\n",
    "avg_rt = sum(r['runtime'] for r in results) / n\n",
    "\n",
    "print(\n",
    "    f\"\\nTable row: | Centralized | \"\n",
    "    f\"Helpfulness: {avg_help:.2f} | Transparency: {avg_transp:.2f} | Task: {avg_task:.2f} | \"\n",
    "    f\"Answer Relevancy: {avg_relevancy:.2f} | \"\n",
    "    f\"RAG Conf.: {avg_ragconf:.2f} | Runtime: {avg_rt:.2f} |\"\n",
    ")\n",
    "\n",
    "with open(\"centralized_deepeval_guardrail_results_ICAIR.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "print(\"\\nSaved centralized_deepeval_guardrail_results_ICAIR.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa11724",
   "metadata": {},
   "source": [
    "## HYBRID GOVERANCE -- \n",
    "\n",
    "The system (controller logic) dynamically chooses where to insert guardrails/checks, depending on the agent outputs, context, or intermediate metric scores.\n",
    "\n",
    "For example:\n",
    "\n",
    "If RAG retrieval confidence/score is high,  skip the summarization check.\n",
    "\n",
    "If RAG is weak,  add extra checks or reroute to summarization for additional validation.\n",
    "How this Hybrid Governance works:\n",
    "Runs full agentic flow (Classifier → RAG → Summarizer).\n",
    "\n",
    "Computes a simple proxy for RAG confidence \n",
    "\n",
    "Conditionally applies guardrails/metrics:\n",
    "\n",
    "If confidence is high: Only run the helpfulness check (fewer, faster, trust downstream less).\n",
    "\n",
    "If confidence is low: Run all checks (transparency & helpfulness).\n",
    "\n",
    "Makes a final decision (accept/retry) based only on the checks run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec531d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Q1: Who was first to sequence a DNA-based genome?\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Gold]: Fred Sanger\n",
      "  [Pred]: In 1977, Fred Sanger sequenced the first DNA-based genome, the 5,386 base-pair Phage Φ-X174 [Source 1].\n",
      "  [Helpfulness]: 1.00 | [Transparency]: 0.10 | [Task]: 1.00 | [Rel]: 1.00 | [RAG Conf]: 0.64 | [Runtime]: 24.48 | [Decision]: accept\n",
      "\n",
      "=== Q2: When was a resolution agreed to about Chinese human rights issues in San Francisco?\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Gold]: April 1, 2008\n",
      "  [Pred]: On April 1, 2008, the San Francisco Board of Supervisors approved a resolution about human rights in China [Source 1] in anticipation of the arrival of the Beijing Olympic torch [Source 1, Source 2].\n",
      "  [Helpfulness]: 1.00 | [Transparency]: 0.00 | [Task]: 1.00 | [Rel]: 1.00 | [RAG Conf]: 0.63 | [Runtime]: 18.97 | [Decision]: accept\n",
      "\n",
      "=== Q3: When did Beyonce sign a letter for ONE Campaign?\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "# --- Hybrid governance: dynamic guardrails based on RAG confidence ---\n",
    "def hybrid_governance(question: str, gold: str, rag_conf_threshold=0.55) -> dict:\n",
    "    t0 = time.time()\n",
    "    # 1. RAG Retrieval\n",
    "    nodes = retriever.retrieve(question)\n",
    "    rag_context = [str(n) for n in nodes]\n",
    "    rag_conf = max(getattr(n, \"score\", 0.0) for n in nodes) if nodes else 0.0\n",
    "\n",
    "    # 2. RAG Generation\n",
    "    rag_answer = rag_generate(question, nodes)\n",
    "\n",
    "    # 3. Summarization (optional)\n",
    "    try:\n",
    "        summary = summarize_answer(rag_answer, question)\n",
    "    except Exception:\n",
    "        summary = rag_answer   # fallback\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    # 4. Evaluation (guardrails selected dynamically)\n",
    "    test_case = LLMTestCase(\n",
    "        input=question,\n",
    "        actual_output=summary,\n",
    "        expected_output=gold,\n",
    "        retrieval_context=rag_context,\n",
    "        tools_called=[], expected_tools=[]\n",
    "    )\n",
    "    output = {\n",
    "        \"query\": question,\n",
    "        \"gold_answer\": gold,\n",
    "        \"response\": summary,\n",
    "        \"retrieved_context\": rag_context,\n",
    "        \"rag_confidence\": rag_conf,\n",
    "        \"runtime\": t1-t0,\n",
    "        \"helpfulness\": float(helpfulness_metric.measure(test_case)),\n",
    "        \"transparency\": float(transparency_metric.measure(test_case)),\n",
    "        \"task_completion\": float(task_metric.measure(test_case)),\n",
    "        \"answer_relevancy\": float(metric_answer_relevancy.measure(test_case)),\n",
    "    }\n",
    "\n",
    "    # --- Guardrail logic ---\n",
    "    if rag_conf >= rag_conf_threshold:\n",
    "        # High confidence: only require helpfulness\n",
    "        passed = output[\"helpfulness\"] >= helpfulness_metric.threshold\n",
    "    else:\n",
    "        # Low confidence: stricter; must pass all\n",
    "        passed = (\n",
    "            output[\"helpfulness\"] >= helpfulness_metric.threshold and\n",
    "            output[\"transparency\"] >= transparency_metric.threshold and\n",
    "            output[\"task_completion\"] >= task_metric.threshold\n",
    "        )\n",
    "    output[\"decision\"] = \"accept\" if passed else \"retry\"\n",
    "    return output\n",
    "\n",
    "# --- Batch runner for hybrid ---\n",
    "results = []\n",
    "for i, (q, g) in enumerate(zip(selected_questions, selected_golds)):\n",
    "    print(f\"\\n=== Q{i+1}: {q}\\n{'='*40}\")\n",
    "    out = hybrid_governance(q, g, rag_conf_threshold=0.55)\n",
    "    print(f\"  [Gold]: {g}\")\n",
    "    print(f\"  [Pred]: {out['response']}\")\n",
    "    print(\n",
    "        f\"  [Helpfulness]: {out['helpfulness']:.2f} | [Transparency]: {out['transparency']:.2f} | \"\n",
    "        f\"[Task]: {out['task_completion']:.2f} | [Rel]: {out['answer_relevancy']:.2f} | \"\n",
    "        f\"[RAG Conf]: {out['rag_confidence']:.2f} | \"\n",
    "        f\"[Runtime]: {out['runtime']:.2f} | [Decision]: {out['decision']}\"\n",
    "    )\n",
    "    results.append(out)\n",
    "\n",
    "# --- Print summary table ---\n",
    "n = len(results)\n",
    "avg_help = sum(r['helpfulness'] for r in results) / n\n",
    "avg_transp = sum(r['transparency'] for r in results) / n\n",
    "avg_task = sum(r['task_completion'] for r in results) / n\n",
    "avg_relevancy = sum(r['answer_relevancy'] for r in results) / n\n",
    "avg_ragconf = sum(r['rag_confidence'] for r in results) / n\n",
    "avg_rt = sum(r['runtime'] for r in results) / n\n",
    "\n",
    "print(\n",
    "    f\"\\nTable row: | Hybrid | \"\n",
    "    f\"Helpfulness: {avg_help:.2f} | Transparency: {avg_transp:.2f} | Task: {avg_task:.2f} | \"\n",
    "    f\"Answer Relevancy: {avg_relevancy:.2f} | \"\n",
    "    f\"RAG Conf.: {avg_ragconf:.2f} | Runtime: {avg_rt:.2f} |\"\n",
    ")\n",
    "with open(\"hybrid_deepeval_guardrail_results_ICAIR.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "print(\"\\nSaved hybrid_deepeval_guardrail_results_ICAIR.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ae4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Batch runner for hybrid with branch/rate debugging ---\n",
    "results = []\n",
    "for i, (q, g) in enumerate(zip(selected_questions, selected_golds)):\n",
    "    t0 = time.time()\n",
    "    out = hybrid_governance(q, g, rag_conf_threshold=0.55)\n",
    "    which_branch = \"FAST\" if out[\"rag_confidence\"] >= 0.55 else \"SLOW\"\n",
    "    print(\n",
    "        f\"Q{i+1}: [Decision]: {out['decision']} | [RAG Conf]: {out['rag_confidence']:.2f} | \"\n",
    "        f\"[Branch]: {which_branch} | [Runtime]: {out['runtime']:.2f} s\"\n",
    "    )\n",
    "    results.append(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ed1ef",
   "metadata": {},
   "source": [
    "## Final Aggregation and Table Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0534d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def get_metrics_from_file(filename, mode=\"centralized\"):\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "    # Handle wrapper dict for each row if present\n",
    "    if mode == \"centralized\":\n",
    "        outs = [row[\"centralized\"] for row in data]\n",
    "    elif mode == \"hybrid\":\n",
    "        outs = [row[\"hybrid\"] for row in data]\n",
    "    else:\n",
    "        outs = data  # distributed\n",
    "    # Compute metrics\n",
    "    accuracy = np.mean([row.get(\"task_completion\", 0) for row in outs])\n",
    "    helpfulness = np.mean([row.get(\"helpfulness\", 0) for row in outs])\n",
    "    transparency = np.mean([row.get(\"transparency\", 0) for row in outs])\n",
    "    rag_conf = np.mean([row.get(\"rag_confidence\", 0) for row in outs])\n",
    "    runtime = np.mean([row.get(\"time\", row.get(\"runtime\", 0)) for row in outs])\n",
    "    accept = sum(1 for row in outs if row.get(\"decision\", \"\").lower() == \"accept\")\n",
    "    retry = sum(1 for row in outs if row.get(\"decision\", \"\").lower() == \"retry\")\n",
    "    n = len(outs)\n",
    "    decision_str = f\"{accept} Accept, {retry} Retry\"\n",
    "    #return accuracy, helpfulness, transparency, rag_conf, runtime, decision_str\n",
    "    return  helpfulness, transparency, rag_conf, runtime\n",
    "\n",
    "# Load metrics for all three\n",
    "centralized_metrics = get_metrics_from_file(\"centralized_deepeval_guardrail_results_ICAIR.json\", mode=\"centralized\")\n",
    "hybrid_metrics = get_metrics_from_file(\"hybrid_deepeval_guardrail_results_ICAIR.json\", mode=\"hybrid\")\n",
    "distributed_metrics = get_metrics_from_file(\"distributed_deepeval_guardrail_results_ICAIR.json\", mode=\"distributed\")\n",
    "\n",
    "# Print LaTeX table rows\n",
    "for name, vals in zip(\n",
    "    [\"Centralized\", \"Hybrid\", \"Distributed\"],\n",
    "    [centralized_metrics, hybrid_metrics, distributed_metrics]\n",
    "):\n",
    "    #print(f\"{name} & {vals[0]:.2f} & {vals[1]:.2f} & {vals[2]:.2f} & {vals[3]:.2f} & {vals[4]:.2f} & {vals[5]} \\\\\\\\\")\n",
    "    print(f\"{name} & {vals[0]:.2f} & {vals[1]:.2f} & {vals[2]:.2f} & {vals[3]:.2f}  \\\\\\\\\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal_multi_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
