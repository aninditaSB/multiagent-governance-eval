{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53b1f51c",
   "metadata": {},
   "source": [
    "\n",
    "## Multi-Agent System on SQuAD with Centralized, Distributed, and Hybrid Governance\n",
    "\n",
    "This notebook contains experiments for a multi-agent system built on the SQuAD dataset. It explores how different governance strategies—**centralized**, **distributed**, and **hybrid**—affect the performance and safety of large language model (LLM)-based agents.\n",
    "\n",
    "### About the Dataset: Why SQuAD?\n",
    "\n",
    "We chose the **Stanford Question Answering Dataset (SQuAD)** because it’s a trusted benchmark for evaluating question-answering systems. It contains over 100,000 questions paired with Wikipedia passages and gold-standard answers.\n",
    "\n",
    "SQuAD is ideal for testing multi-agent systems because:\n",
    "\n",
    "* It covers a wide variety of factual, text-based questions.\n",
    "* Answers are context-grounded and span-based, making evaluation easier and more accurate.\n",
    "* It reflects tasks like retrieval, reasoning, and summarization—core skills for agentic systems.\n",
    "\n",
    "### System Design: The Three Core Agents\n",
    "\n",
    "Our system breaks down the QA task into three modular agents:\n",
    "\n",
    "* **ClassifierAgent** – Detects the question type (e.g., who, what, when) to guide how the question is handled.\n",
    "* **RAGAgent** – Uses retrieval-augmented generation to find relevant information and generate an answer.\n",
    "* **SummarizerAgent** – (Optional) Summarizes longer answers for clarity and readability.\n",
    "\n",
    "This design reflects how people solve complex problems—by breaking them into focused steps—and allows us to analyze each part individually.\n",
    "\n",
    "### Governance Architectures: How Control is Applied\n",
    "\n",
    "We implement and compare three governance models for managing agent behavior:\n",
    "\n",
    "#### 1. Centralized Governance\n",
    "\n",
    "* **How it works**: The system completes all agent steps, then runs quality checks only at the end.\n",
    "* **Key feature**: A single controller reviews the final answer, like a teacher grading a test.\n",
    "* **Purpose**: Acts as a baseline to measure the effects of having full control at one point.\n",
    "\n",
    "#### 2. Distributed Governance\n",
    "\n",
    "* **How it works**: Each agent checks its own output before passing it on. If the output fails, it retries or stops the process.\n",
    "* **Key feature**: There’s no central controller—each agent is responsible for its own quality.\n",
    "* **Purpose**: Models decentralized, peer-like agent systems where each module is self-accountable.\n",
    "\n",
    "#### 3. Hybrid Governance\n",
    "\n",
    "* **How it works**: A central controller decides when and where to apply checks based on confidence scores.\n",
    "\n",
    "  * If the system is confident, it runs fewer checks.\n",
    "  * If uncertain, it adds extra evaluations (e.g., transparency or summarization).\n",
    "* **Key feature**: A balance between the other two models—adaptive, not fixed.\n",
    "* **Purpose**: Reflects real-world systems that optimize resources by checking more only when needed.\n",
    "\n",
    "### Evaluation and Guardrails\n",
    "\n",
    "We used **DeepEval** to implement guardrails—automated quality checks—for each governance mode. These guardrails evaluated:\n",
    "\n",
    "* Answer relevance\n",
    "* Task completion\n",
    "* Transparency\n",
    "* Helpfulness\n",
    "\n",
    "In **distributed** and **hybrid** modes, these metrics were applied at the agent level, allowing dynamic responses like retrying or escalating low-confidence answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe7f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import nest_asyncio; nest_asyncio.apply()  # Needed for notebook async loop fixes\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.tools import QueryEngineTool, FunctionTool\n",
    "from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e45c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GOOGLE_API_KEY\")\n",
    "assert GOOGLE_API_KEY, \"Set your Google API key!\"\n",
    "\n",
    "# LLM and Embedding Model\n",
    "llm = GoogleGenAI(model=\"models/gemini-2.5-pro\", api_key=GOOGLE_API_KEY)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c72d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DeepEval Metrics ---\n",
    "from deepeval.models import GeminiModel\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.models import GeminiModel\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    GEval,\n",
    "    TaskCompletionMetric\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e882deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepEval model/metrics\n",
    "eval_model = GeminiModel(model_name=\"gemini-2.5-pro\", api_key=GOOGLE_API_KEY)\n",
    "transparency_metric = GEval(\n",
    "    model=eval_model,\n",
    "    name=\"Transparency\",\n",
    "    criteria=\"Does the answer clearly explain its reasoning steps and tool usage? Rate 0.0–1.0.\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT],\n",
    "    threshold=0.35,\n",
    ")\n",
    "helpfulness_metric = GEval(\n",
    "    model=eval_model,\n",
    "    name=\"Helpfulness\",\n",
    "    criteria=\"How helpful is this answer to the user's question? Rate 0.0-1.0.\",\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.35,\n",
    ")\n",
    "task_metric = TaskCompletionMetric(model=eval_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81cfbc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_answer_relevancy = AnswerRelevancyMetric(model=eval_model, threshold=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951adc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and shuffle SQuAD train split\n",
    "#full_train = load_dataset(\"squad\", split=\"train[:10000]\").shuffle(seed=42)\n",
    "\n",
    "#eval_size = 1000  # 10% for eval\n",
    "\n",
    "full_train = load_dataset(\"squad\", split=\"train[:12000]\").shuffle(seed=42)\n",
    "eval_size = 2000  # Now using 2,000 for evaluation\n",
    "\n",
    "\n",
    "index_size = len(full_train) - eval_size\n",
    "\n",
    "# 2. Build RAG corpus from first 9000 (use select)\n",
    "rag_docs = [Document(text=d[\"context\"]) for d in full_train.select(range(index_size))]\n",
    "\n",
    "# 3. Build eval set from held-out 1000\n",
    "eval_examples = full_train.select(range(index_size, len(full_train)))\n",
    "\n",
    "questions = [d[\"question\"] for d in eval_examples]\n",
    "golds = [d[\"answers\"][\"text\"][0] if d[\"answers\"][\"text\"] else \"\" for d in eval_examples]\n",
    "eval_contexts = [d[\"context\"] for d in eval_examples]  # For debugging\n",
    "\n",
    "print(f\"RAG corpus size: {len(rag_docs)} | Eval set: {len(questions)} Qs (all context in index)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f2b0686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RAG index and retriever ---\n",
    "docs_index = VectorStoreIndex.from_documents(rag_docs, embed_model=embed_model)\n",
    "query_engine = docs_index.as_query_engine(llm=llm)\n",
    "rag_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine, name=\"RAGAgent\", description=\"Retrieval-augmented generation for SQuAD.\"\n",
    ")\n",
    "retriever = docs_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e20dca",
   "metadata": {},
   "source": [
    "# DISTRIBUTED - independent agents with guardrails at all agent levels - no central controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dda5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nest_asyncio; nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2177a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Build RAG index and query engine ---\n",
    "#docs_index = VectorStoreIndex.from_documents(rag_docs, embed_model=embed_model)\n",
    "retriever = docs_index.as_retriever(similarity_top_k=10)  # Increased top_k for more/better context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25956681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_prompt(question, context_snippets):\n",
    "    context = \"\\n\".join([f\"[Source {i+1}]: {snippet}\" for i, snippet in enumerate(context_snippets)])\n",
    "    prompt = f'''\n",
    "You are a question answering system. Use the information in the SOURCES below to answer the question as completely and accurately as possible.\n",
    "- Cite supporting facts with [Source #] after each claim.\n",
    "- If the answer cannot be found in the sources, reply: \"Not found in context.\"\n",
    "- Keep your answer concise but complete; do not copy sources verbatim unless necessary.\n",
    "\n",
    "SOURCES:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER (with citations):\n",
    "'''\n",
    "    return prompt\n",
    "\n",
    "def verify_citations(answer, context_snippets):\n",
    "    import re\n",
    "    for match in re.finditer(r\"\\[Source (\\d+)\\]\", answer):\n",
    "        idx = int(match.group(1)) - 1\n",
    "        if idx < 0 or idx >= len(context_snippets):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def deepeval_guardrail(answer: str, question: str, retrieval_context: list = None):\n",
    "    test_case = LLMTestCase(\n",
    "        input=question,\n",
    "        actual_output=answer,\n",
    "        retrieval_context=retrieval_context or []\n",
    "    )\n",
    "    helpfulness = float(helpfulness_metric.measure(test_case))\n",
    "    answer_relevancy = float(metric_answer_relevancy.measure(test_case))\n",
    "    citations_ok = verify_citations(answer, retrieval_context or [])\n",
    "    if helpfulness < 0.35:\n",
    "        return f\"FAIL: Helpfulness below threshold ({helpfulness:.2f})\"\n",
    "    if answer_relevancy < 0.8:\n",
    "        return f\"FAIL: AnswerRelevancy below threshold ({answer_relevancy:.2f})\"\n",
    "    if not citations_ok:\n",
    "        return \"FAIL: Citation(s) do not match provided sources\"\n",
    "    return \"PASS\"\n",
    "\n",
    "def rag_generate(question, retrieved_nodes):\n",
    "    context_snippets = [str(n) for n in retrieved_nodes]\n",
    "    prompt = rag_prompt(question, context_snippets)\n",
    "    response = llm.complete(prompt)\n",
    "    if hasattr(response, 'text'):\n",
    "        return response.text\n",
    "    return str(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c12308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "# 1. Normalize text and robustly check for answer in context\n",
    "def normalize_text(text):\n",
    "    \"\"\"Lowercase, remove punctuation, extra whitespace.\"\"\"\n",
    "    return re.sub(r'[\\W_]+', ' ', text.lower()).strip()\n",
    "\n",
    "def answer_in_context(gold, rag_context, min_frac=0.7):\n",
    "    \"\"\"\n",
    "    Returns True if gold answer is present (even partially) in any context snippet.\n",
    "    min_frac: minimum fraction of gold words found in context for robust match.\n",
    "    \"\"\"\n",
    "    gold_norm = normalize_text(gold)\n",
    "    gold_words = set(gold_norm.split())\n",
    "    if not gold_words:\n",
    "        return False\n",
    "    for snippet in rag_context:\n",
    "        snippet_norm = normalize_text(snippet)\n",
    "        # 1. Exact match (normalized)\n",
    "        if gold_norm in snippet_norm:\n",
    "            return True\n",
    "        # 2. Loose/partial match: most gold words are in context\n",
    "        snippet_words = set(snippet_norm.split())\n",
    "        common = gold_words & snippet_words\n",
    "        if len(common) / len(gold_words) >= min_frac:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 2. Filter eval set for only those Qs with gold answer in retrieved context\n",
    "selected_questions, selected_golds = [], []\n",
    "for i, (q, g) in enumerate(zip(questions, golds)):\n",
    "    nodes = retriever.retrieve(q)\n",
    "    rag_context = [str(n) for n in nodes]\n",
    "    if answer_in_context(g, rag_context, min_frac=0.7):\n",
    "        selected_questions.append(q)\n",
    "        selected_golds.append(g)\n",
    "        print(f\"[OK] Q{i+1} added. Gold answer is present in context.\")\n",
    "    else:\n",
    "        print(f\"[SKIP] Q{i+1}: Gold NOT found in context.\\nGold: {g}\\n\")\n",
    "    #if len(selected_questions) >= 20:   # <-- How many eval Qs you want\n",
    "    #    break\n",
    "    if len(selected_questions) >= 50:   # <-- How many eval Qs you want\n",
    "        break\n",
    "\n",
    "print(f\"\\nSelected {len(selected_questions)} eval Qs out of {len(questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bec25f",
   "metadata": {},
   "source": [
    "#run for 50 selected questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f0cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Run distributed governance on the filtered eval set\n",
    "\n",
    "async def run_distributed_squad(n=None, max_retries=1, rag_conf_threshold=0.35):\n",
    "    results = []\n",
    "    initial_top_k = retriever.similarity_top_k  # Save at start\n",
    "    use_n = n if n else len(selected_questions)\n",
    "\n",
    "    # Define all metrics (including new ones)\n",
    "    metrics = {\n",
    "        \"helpfulness\": helpfulness_metric,\n",
    "        \"transparency\": transparency_metric,\n",
    "        \"task_completion\": task_metric,\n",
    "        \"answer_relevancy\": metric_answer_relevancy,\n",
    "        \n",
    "    }\n",
    "\n",
    "    for i, (q, g) in enumerate(zip(selected_questions, selected_golds)):\n",
    "        if i >= use_n:\n",
    "            break\n",
    "        print(f\"\\n=== Q{i+1}: {q}\\n{'='*40}\")\n",
    "        t0 = time.time()\n",
    "        retries = 0\n",
    "        agent_answer = None\n",
    "        rag_context = []\n",
    "        rag_conf = 0.0\n",
    "        decision = \"retry\"\n",
    "        last_top_k = retriever.similarity_top_k\n",
    "\n",
    "        while decision != \"accept\" and retries <= max_retries:\n",
    "            step_start = time.time()\n",
    "            # ---- Retrieval Step ----\n",
    "            nodes = retriever.retrieve(q)\n",
    "            rag_context = [str(n) for n in nodes]\n",
    "            rag_conf = max(getattr(n, \"score\", 0.0) for n in nodes) if nodes else 0.0\n",
    "            print(f\"  [Retry {retries}] RAG context retrieved, confidence: {rag_conf:.2f}\")\n",
    "\n",
    "            print(\"    [Context snippets]:\")\n",
    "            for idx, snippet in enumerate(rag_context):\n",
    "                snippet_preview = snippet[:180].replace('\\n', ' ')\n",
    "                print(f\"      [{idx+1}] {snippet_preview}...\")\n",
    "\n",
    "            # Dynamically increase top_k, but reset per question\n",
    "            if rag_conf < rag_conf_threshold and retries < max_retries:\n",
    "                retriever.similarity_top_k += 2\n",
    "                if retriever.similarity_top_k != last_top_k:\n",
    "                    print(f\"    [Info] Increased retriever.similarity_top_k to {retriever.similarity_top_k}\")\n",
    "                    last_top_k = retriever.similarity_top_k\n",
    "\n",
    "            # ---- RAG Agent Step ----\n",
    "            agent_start = time.time()\n",
    "            rag_prompt_str = rag_prompt(q, rag_context)\n",
    "            print(f\"    [RAG Prompt to LLM]:\\n{rag_prompt_str[:600]}...\\n--- END PROMPT ---\")\n",
    "            agent_answer = rag_generate(q, nodes)\n",
    "            agent_end = time.time()\n",
    "            print(f\"    [RAG LLM] Agent answer generated in {agent_end-agent_start:.2f}s\")\n",
    "            print(f\"    [LLM Answer]:\\n{str(agent_answer)[:400]}...\\n--- END ANSWER ---\")\n",
    "            \n",
    "            # ---- Guardrail Step ----\n",
    "            guardrail_start = time.time()\n",
    "            guardrail_result = deepeval_guardrail(agent_answer, q, rag_context)\n",
    "            guardrail_end = time.time()\n",
    "            print(f\"    [Guardrail] Result: {guardrail_result} (checked in {guardrail_end-guardrail_start:.2f}s)\")\n",
    "            if guardrail_result == \"PASS\" and rag_conf >= rag_conf_threshold:\n",
    "                decision = \"accept\"\n",
    "            else:\n",
    "                print(f\"    [Retry] Guardrail failed or RAG conf too low; retrying...\")\n",
    "                retries += 1\n",
    "            step_end = time.time()\n",
    "            print(f\"  [Retry {retries}] Step runtime: {step_end-step_start:.2f}s\")\n",
    "\n",
    "        t1 = time.time()\n",
    "        retriever.similarity_top_k = initial_top_k  # Reset for next question\n",
    "        print(f\"    [Gold Answer]: {g}\\n\")\n",
    "\n",
    "        # ---- Metrics Calculation ----\n",
    "        test_case = LLMTestCase(\n",
    "            input=q,\n",
    "            actual_output=agent_answer,\n",
    "            expected_output=g,\n",
    "            retrieval_context=rag_context,\n",
    "            tools_called=[], expected_tools=[]\n",
    "        )\n",
    "        metric_scores = {}\n",
    "        for name, metric in metrics.items():\n",
    "            score = float(metric.measure(test_case))\n",
    "            metric_scores[name] = score\n",
    "        metric_str = \", \".join(f\"{k}: {v:.2f}\" for k, v in metric_scores.items())\n",
    "        print(f\"    [Metrics] {metric_str}\")\n",
    "\n",
    "        results.append({\n",
    "            \"index\": i+1,\n",
    "            \"query\": q,\n",
    "            \"gold_answer\": g,\n",
    "            \"agent_answer\": agent_answer,\n",
    "            \"retrieved_context\": rag_context,\n",
    "            \"rag_confidence\": rag_conf,\n",
    "            \"decision\": decision,\n",
    "            \"runtime\": t1 - t0,\n",
    "            **metric_scores  # Inject all metric values directly into the result dict\n",
    "        })\n",
    "        print(f\"Q{i+1} done. RAG Conf: {rag_conf:.2f} (total {t1-t0:.2f}s)\")\n",
    "\n",
    "    # ---- Table Stats (Dynamic Metrics) ----\n",
    "    avg_metrics = {\n",
    "        name: sum(d[name] for d in results) / len(results)\n",
    "        for name in metrics\n",
    "    }\n",
    "    avg_ragconf = sum(d['rag_confidence'] for d in results) / len(results)\n",
    "    avg_rt = sum(d['runtime'] for d in results) / len(results)\n",
    "\n",
    "    metric_values_str = \" | \".join(f\"{m}: {avg_metrics[m]:.2f}\" for m in metrics)\n",
    "    print(f\"\\nTable row: | Distributed | {metric_values_str} | RAG Conf.: {avg_ragconf:.2f} | Runtime: {avg_rt:.2f} |\")\n",
    "\n",
    "    # ---- Save Results ----\n",
    "    with open(\"distributed_deepeval_guardrail_results_ICAIR.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    print(\"\\nSaved distributed_deepeval_guardrail_results_ICAIR.json\")\n",
    "\n",
    "# 4. RUN!\n",
    "await run_distributed_squad(max_retries=1, rag_conf_threshold=0.35) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a6ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load  results JSON\n",
    "with open(\"distributed_deepeval_guardrail_results_ICAIR.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# The metrics to average\n",
    "metrics = [\n",
    "    \"helpfulness\",\n",
    "    \"transparency\",\n",
    "    \"task_completion\",\n",
    "    \"answer_relevancy\",\n",
    "    \"rag_confidence\",\n",
    "    \"runtime\"\n",
    "]\n",
    "\n",
    "# Compute averages (protect against missing keys)\n",
    "avg = {}\n",
    "N = len(results)\n",
    "for m in metrics:\n",
    "    vals = [r.get(m, 0.0) for r in results]\n",
    "    avg[m] = sum(vals) / N if N else 0.0\n",
    "\n",
    "# Print in table format\n",
    "print(\"==== Final Results (Averages over {} samples) ====\".format(N))\n",
    "print(\"Helpfulness: {:.2f}\".format(avg[\"helpfulness\"]))\n",
    "print(\"Transparency: {:.2f}\".format(avg[\"transparency\"]))\n",
    "print(\"Task Completion: {:.2f}\".format(avg[\"task_completion\"]))\n",
    "print(\"Answer Relevancy: {:.2f}\".format(avg[\"answer_relevancy\"]))\n",
    "print(\"Avg. RAG Conf.: {:.2f}\".format(avg[\"rag_confidence\"]))\n",
    "print(\"Avg. Runtime (s): {:.2f}\".format(avg[\"runtime\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c261f",
   "metadata": {},
   "source": [
    "\n",
    "# CENTRALIZED GOVERANCE\n",
    "- Centralised governance = run the agentic flow end-to-end, get the final eval and decide what to do.\n",
    "- centralized is like a teacher evaluting our final output of all agents and the way they solved the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5a6f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer_prompt(answer_with_sources, question):\n",
    "    return (\n",
    "        \"Summarize the answer below, preserving all source citations.\\n\\n\"\n",
    "        f\"Answer: {answer_with_sources}\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        \"Summary:\"\n",
    "    )\n",
    "\n",
    "def summarize_answer(answer_with_sources, question):\n",
    "    \"\"\"\n",
    "    Summarizes the RAG answer, preserving citations and using the original question for clarity.\n",
    "    \"\"\"\n",
    "    prompt = summarizer_prompt(answer_with_sources, question)\n",
    "    response = llm.complete(prompt)\n",
    "    if hasattr(response, 'text'):\n",
    "        return response.text\n",
    "    return str(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4432742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "# --- Centralized governance (run full pipeline then evaluate once) ---\n",
    "def centralized_governance(question: str, gold: str) -> dict:\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # 1. RAG Retrieval\n",
    "    nodes = retriever.retrieve(question)\n",
    "    rag_context = [str(n) for n in nodes]\n",
    "    rag_conf = max(getattr(n, \"score\", 0.0) for n in nodes) if nodes else 0.0\n",
    "\n",
    "    # 2. RAG Generation\n",
    "    rag_answer = rag_generate(question, nodes)\n",
    "\n",
    "    # 3. Summarization (if you want to summarize before evaluation)\n",
    "    try:\n",
    "        summary = summarize_answer(rag_answer, question)\n",
    "    except Exception:\n",
    "        summary = rag_answer   # fallback\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    # 4. Evaluation (centralized, on summary)\n",
    "    test_case = LLMTestCase(\n",
    "        input=question,\n",
    "        actual_output=summary,\n",
    "        expected_output=gold,\n",
    "        retrieval_context=rag_context,\n",
    "        tools_called=[], expected_tools=[]\n",
    "    )\n",
    "    output = {\n",
    "        \"query\": question,\n",
    "        \"gold_answer\": gold,\n",
    "        \"response\": summary,\n",
    "        \"retrieved_context\": rag_context,\n",
    "        \"rag_confidence\": rag_conf,\n",
    "        \"runtime\": t1-t0,\n",
    "        \"helpfulness\": float(helpfulness_metric.measure(test_case)),\n",
    "        \"transparency\": float(transparency_metric.measure(test_case)),\n",
    "        \"task_completion\": float(task_metric.measure(test_case)),\n",
    "        \"answer_relevancy\": float(metric_answer_relevancy.measure(test_case)),\n",
    "    }\n",
    "    \n",
    "    # All-or-nothing guardrail (centralized)\n",
    "    passed = (\n",
    "        output[\"helpfulness\"] >= helpfulness_metric.threshold and\n",
    "        output[\"transparency\"] >= transparency_metric.threshold and\n",
    "        output[\"task_completion\"] >= task_metric.threshold\n",
    "    )\n",
    "    output[\"decision\"] = \"accept\" if passed else \"retry\"\n",
    "    \n",
    "    return output\n",
    "\n",
    "# ---- Batch runner ----\n",
    "results = []\n",
    "for i, (q, g) in enumerate(zip(selected_questions, selected_golds)):\n",
    "    print(f\"\\n=== Q{i+1}: {q}\\n{'='*40}\")\n",
    "    out = centralized_governance(q, g)\n",
    "    print(f\"  [Gold]: {g}\")\n",
    "    print(f\"  [Pred]: {out['response']}\")\n",
    "    print(\n",
    "        f\"  [Help]: {out['helpfulness']:.2f} | [Transp]: {out['transparency']:.2f} | \"\n",
    "        f\"[Task]: {out['task_completion']:.2f} | [Rel]: {out['answer_relevancy']:.2f} | \"\n",
    "        f\"[RAG Conf]: {out['rag_confidence']:.2f} | [Decision]: {out['decision']}\"\n",
    "    )\n",
    "    results.append(out)\n",
    "\n",
    "# --- Print summary table ---\n",
    "n = len(results)\n",
    "avg_help = sum(r['helpfulness'] for r in results) / n\n",
    "avg_transp = sum(r['transparency'] for r in results) / n\n",
    "avg_task = sum(r['task_completion'] for r in results) / n\n",
    "avg_relevancy = sum(r['answer_relevancy'] for r in results) / n\n",
    "avg_ragconf = sum(r['rag_confidence'] for r in results) / n\n",
    "avg_rt = sum(r['runtime'] for r in results) / n\n",
    "\n",
    "print(\n",
    "    f\"\\nTable row: | Centralized | \"\n",
    "    f\"Helpfulness: {avg_help:.2f} | Transparency: {avg_transp:.2f} | Task: {avg_task:.2f} | \"\n",
    "    f\"Answer Relevancy: {avg_relevancy:.2f} | \"\n",
    "    f\"RAG Conf.: {avg_ragconf:.2f} | Runtime: {avg_rt:.2f} |\"\n",
    ")\n",
    "\n",
    "with open(\"centralized_deepeval_guardrail_results_ICAIR.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "print(\"\\nSaved centralized_deepeval_guardrail_results_ICAIR.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa11724",
   "metadata": {},
   "source": [
    "## HYBRID GOVERANCE -- \n",
    "\n",
    "The system (controller logic) dynamically chooses where to insert guardrails/checks, depending on the agent outputs, context, or intermediate metric scores.\n",
    "\n",
    "For example:\n",
    "\n",
    "If RAG retrieval confidence/score is high,  skip the summarization check.\n",
    "\n",
    "If RAG is weak,  add extra checks or reroute to summarization for additional validation.\n",
    "How this Hybrid Governance works:\n",
    "Runs full agentic flow (Classifier → RAG → Summarizer).\n",
    "\n",
    "Computes a simple proxy for RAG confidence \n",
    "\n",
    "Conditionally applies guardrails/metrics:\n",
    "\n",
    "If confidence is high: Only run the helpfulness check (fewer, faster, trust downstream less).\n",
    "\n",
    "If confidence is low: Run all checks (transparency & helpfulness).\n",
    "\n",
    "Makes a final decision (accept/retry) based only on the checks run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec531d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "# --- Hybrid governance: dynamic guardrails based on RAG confidence ---\n",
    "def hybrid_governance(question: str, gold: str, rag_conf_threshold=0.55) -> dict:\n",
    "    t0 = time.time()\n",
    "    # 1. RAG Retrieval\n",
    "    nodes = retriever.retrieve(question)\n",
    "    rag_context = [str(n) for n in nodes]\n",
    "    rag_conf = max(getattr(n, \"score\", 0.0) for n in nodes) if nodes else 0.0\n",
    "\n",
    "    # 2. RAG Generation\n",
    "    rag_answer = rag_generate(question, nodes)\n",
    "\n",
    "    # 3. Summarization (optional)\n",
    "    try:\n",
    "        summary = summarize_answer(rag_answer, question)\n",
    "    except Exception:\n",
    "        summary = rag_answer   # fallback\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    # 4. Evaluation (guardrails selected dynamically)\n",
    "    test_case = LLMTestCase(\n",
    "        input=question,\n",
    "        actual_output=summary,\n",
    "        expected_output=gold,\n",
    "        retrieval_context=rag_context,\n",
    "        tools_called=[], expected_tools=[]\n",
    "    )\n",
    "    output = {\n",
    "        \"query\": question,\n",
    "        \"gold_answer\": gold,\n",
    "        \"response\": summary,\n",
    "        \"retrieved_context\": rag_context,\n",
    "        \"rag_confidence\": rag_conf,\n",
    "        \"runtime\": t1-t0,\n",
    "        \"helpfulness\": float(helpfulness_metric.measure(test_case)),\n",
    "        \"transparency\": float(transparency_metric.measure(test_case)),\n",
    "        \"task_completion\": float(task_metric.measure(test_case)),\n",
    "        \"answer_relevancy\": float(metric_answer_relevancy.measure(test_case)),\n",
    "    }\n",
    "\n",
    "    # --- Guardrail logic ---\n",
    "    if rag_conf >= rag_conf_threshold:\n",
    "        # High confidence: only require helpfulness\n",
    "        passed = output[\"helpfulness\"] >= helpfulness_metric.threshold\n",
    "    else:\n",
    "        # Low confidence: stricter; must pass all\n",
    "        passed = (\n",
    "            output[\"helpfulness\"] >= helpfulness_metric.threshold and\n",
    "            output[\"transparency\"] >= transparency_metric.threshold and\n",
    "            output[\"task_completion\"] >= task_metric.threshold\n",
    "        )\n",
    "    output[\"decision\"] = \"accept\" if passed else \"retry\"\n",
    "    return output\n",
    "\n",
    "# --- Batch runner for hybrid ---\n",
    "results = []\n",
    "for i, (q, g) in enumerate(zip(selected_questions, selected_golds)):\n",
    "    print(f\"\\n=== Q{i+1}: {q}\\n{'='*40}\")\n",
    "    out = hybrid_governance(q, g, rag_conf_threshold=0.55)\n",
    "    print(f\"  [Gold]: {g}\")\n",
    "    print(f\"  [Pred]: {out['response']}\")\n",
    "    print(\n",
    "        f\"  [Helpfulness]: {out['helpfulness']:.2f} | [Transparency]: {out['transparency']:.2f} | \"\n",
    "        f\"[Task]: {out['task_completion']:.2f} | [Rel]: {out['answer_relevancy']:.2f} | \"\n",
    "        f\"[RAG Conf]: {out['rag_confidence']:.2f} | \"\n",
    "        f\"[Runtime]: {out['runtime']:.2f} | [Decision]: {out['decision']}\"\n",
    "    )\n",
    "    results.append(out)\n",
    "\n",
    "# --- Print summary table ---\n",
    "n = len(results)\n",
    "avg_help = sum(r['helpfulness'] for r in results) / n\n",
    "avg_transp = sum(r['transparency'] for r in results) / n\n",
    "avg_task = sum(r['task_completion'] for r in results) / n\n",
    "avg_relevancy = sum(r['answer_relevancy'] for r in results) / n\n",
    "avg_ragconf = sum(r['rag_confidence'] for r in results) / n\n",
    "avg_rt = sum(r['runtime'] for r in results) / n\n",
    "\n",
    "print(\n",
    "    f\"\\nTable row: | Hybrid | \"\n",
    "    f\"Helpfulness: {avg_help:.2f} | Transparency: {avg_transp:.2f} | Task: {avg_task:.2f} | \"\n",
    "    f\"Answer Relevancy: {avg_relevancy:.2f} | \"\n",
    "    f\"RAG Conf.: {avg_ragconf:.2f} | Runtime: {avg_rt:.2f} |\"\n",
    ")\n",
    "with open(\"hybrid_deepeval_guardrail_results_ICAIR.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "print(\"\\nSaved hybrid_deepeval_guardrail_results_ICAIR.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ae4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Batch runner for hybrid with branch/rate debugging ---\n",
    "results = []\n",
    "for i, (q, g) in enumerate(zip(selected_questions, selected_golds)):\n",
    "    t0 = time.time()\n",
    "    out = hybrid_governance(q, g, rag_conf_threshold=0.55)\n",
    "    which_branch = \"FAST\" if out[\"rag_confidence\"] >= 0.55 else \"SLOW\"\n",
    "    print(\n",
    "        f\"Q{i+1}: [Decision]: {out['decision']} | [RAG Conf]: {out['rag_confidence']:.2f} | \"\n",
    "        f\"[Branch]: {which_branch} | [Runtime]: {out['runtime']:.2f} s\"\n",
    "    )\n",
    "    results.append(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ed1ef",
   "metadata": {},
   "source": [
    "## Final Aggregation and Table Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0534d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def get_metrics_from_file(filename, mode=\"centralized\"):\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)\n",
    "    # Handle wrapper dict for each row if present\n",
    "    if mode == \"centralized\":\n",
    "        outs = [row[\"centralized\"] for row in data]\n",
    "    elif mode == \"hybrid\":\n",
    "        outs = [row[\"hybrid\"] for row in data]\n",
    "    else:\n",
    "        outs = data  # distributed\n",
    "    # Compute metrics\n",
    "    accuracy = np.mean([row.get(\"task_completion\", 0) for row in outs])\n",
    "    helpfulness = np.mean([row.get(\"helpfulness\", 0) for row in outs])\n",
    "    transparency = np.mean([row.get(\"transparency\", 0) for row in outs])\n",
    "    rag_conf = np.mean([row.get(\"rag_confidence\", 0) for row in outs])\n",
    "    runtime = np.mean([row.get(\"time\", row.get(\"runtime\", 0)) for row in outs])\n",
    "    accept = sum(1 for row in outs if row.get(\"decision\", \"\").lower() == \"accept\")\n",
    "    retry = sum(1 for row in outs if row.get(\"decision\", \"\").lower() == \"retry\")\n",
    "    n = len(outs)\n",
    "    decision_str = f\"{accept} Accept, {retry} Retry\"\n",
    "    #return accuracy, helpfulness, transparency, rag_conf, runtime, decision_str\n",
    "    return  helpfulness, transparency, rag_conf, runtime\n",
    "\n",
    "# Load metrics for all three\n",
    "centralized_metrics = get_metrics_from_file(\"centralized_deepeval_guardrail_results_ICAIR.json\", mode=\"centralized\")\n",
    "hybrid_metrics = get_metrics_from_file(\"hybrid_deepeval_guardrail_results_ICAIR.json\", mode=\"hybrid\")\n",
    "distributed_metrics = get_metrics_from_file(\"distributed_deepeval_guardrail_results_ICAIR.json\", mode=\"distributed\")\n",
    "\n",
    "# Print LaTeX table rows\n",
    "for name, vals in zip(\n",
    "    [\"Centralized\", \"Hybrid\", \"Distributed\"],\n",
    "    [centralized_metrics, hybrid_metrics, distributed_metrics]\n",
    "):\n",
    "    #print(f\"{name} & {vals[0]:.2f} & {vals[1]:.2f} & {vals[2]:.2f} & {vals[3]:.2f} & {vals[4]:.2f} & {vals[5]} \\\\\\\\\")\n",
    "    print(f\"{name} & {vals[0]:.2f} & {vals[1]:.2f} & {vals[2]:.2f} & {vals[3]:.2f}  \\\\\\\\\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal_multi_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
